---
layout: post
title: '机器学习基础—参数估计'
subtitle: '概率论基础'
date: 2019-09-11
categories: HKUST
tags:  MachineLearning
---

[TOC]

关键字：
- 贝叶斯定理
- 极大似然估计
- 最大化后验概率
- 贝叶斯参数估计


> Reference：
> 1. 参数估计之贝叶斯估计：[原文链接](https://blog.csdn.net/qq_32742009/article/details/81481680)
> 2. 极大似然估计与贝叶斯估计: [原文链接](https://blog.csdn.net/liu1194397014/article/details/52766760)

### 统计推断

***

根据样本信息对总体分布或总体的特征数进行推断。

贝叶斯统计中，利用三种信息进行统计推断：

> 1. 总体信息：总体分布或者总体所属分布提供的信息，譬如：“已知总体呈正态分布”。
> 2. 样本信息：所抽取的样本的所有特征信息。
> 3. 先验信息：在实验之前对问题的经验和资料，类条件概率。

### 贝叶斯估计核心问题

定义已有的样本集合为$D$，样本集合$D$中的样本都是从一个固定但是未知的概率密度函数（连续）$p(x)$ 中独立抽取出来的，要求根据这些样本估计$x$的概率分布，记为$p(x\|D)$，并且使得$p(x\|D)$尽可能的接近$p(x)$。

### 预知识

| 符号 | 含义|
| :----: | :------: |
| $D$ | 已有的数据 |
| $\theta$ | 待估计的参数 |
| $p(\theta)$ | 先验概率 |
| $p(\theta\|D)$ | 后验概率 |
| $p(D)$ | 数据分布 |
|$p(D\|\theta)$ | 似然函数 |
| $p(x,\theta\|D)$ | 已知数据条件下的$x, \theta$概率 |

现已有样本集$D=\{x_1, x_2, ... , x_N\}$，我们假设这些数据是以含有未知参数$\theta$的某种概率形式分布的。通过已有的数据，可以对这个未知参数$\theta$进行估计，进而对外来的数据进行预测。

##### 举个例子

假设有一个抛硬币的实验，但我们不知道这些硬币是否正反均匀的，投掷硬币正面向上记为1，概率为$\rho$，反面向上记为0，概率为$(1-\rho)$。进行三次试验后得到了一个数据集$D = \{1, 1, 0\}$，这里，$\theta = \rho$。

### 极大似然估计 - MLE

***

极大似然估计法认为参数$\theta$是固定的，由于一些外界参数的干扰使得数据看起来不是完全由参数决定的。那么，就在数据给定的情况下，找一个概率最大的参数$\theta$作为估计值。

因此，问题变成了一个条件概率最大的求解，即求出使得$p(\theta\|D)$最大的参数$\theta$。

式1:

$$arg \max_\theta p(\theta\,|\,D)$$

根据条件概率公式，有：

式2:

$$p(\theta\, | \,D) = \frac{p(D\, | \,\theta)p(\theta)}{p(D)}$$

在MLE中，$\theta$是固定的，数据集$D$也是已知的，所以$p(\theta) / p(D)$可以看作一个常数，此时式2可以变为求解：

式3:

$$\hat{\theta}_{MLE} = arg \max_\theta p(D\, |\,\theta)$$

上式中$p(D\, \|\,\theta)$也称为似然函数，因此要求的参数$\theta$是一个使得似然性最大的参数。

式4:

$$p(D\, |\,\theta) = \prod_{i=1}^n p(x_i | \theta)$$

##### 实例

沿用上面抛硬币的例子和实验数据，可以将式4写出来：

式5:

$$ p(D\, |\,\theta) = p(1\, |\, \rho) * p(1\, |\, \rho) * p(0\, |\, \rho)$$

$$ = \rho * \rho * (1-\rho)$$

$$ = \rho^2 (1-\rho)^1$$

一般对式4取对数来求解极大似然，可以将连乘变成求和，然后求导取极值来获取使得似然函数最大的参数值$\theta$。

将式5取对数，可以得到：

式6:

$$\log{p(D\, |\,\theta)} = 2*\log{\rho}+1*\log{(1-\rho)}$$

对式6求导，可以得到使得似然函数最大的$\hat{\theta}$值：

![f42c7f7fa55fcc9262955201a50e76b2.jpeg](https://app.yinxiang.com/shard/s8/res/1a2c9b82-92ac-4307-910f-114a992e3c8e/78a759e1-2069-4746-8bdc-8bf8305037d3.jpg)

### 贝叶斯定理

***

首先介绍一些基础概念。这里引入一个例子来辅助说明：事件A：出门遇上堵车，事件B：发生了交通事故。

**先验概率(prior)**：指根据以往经验和分析得到的概率。在以上的例子中，`P(A)`和`P(B)`都是先验概率。

**后验概率(posterior)**：事件发生后求的反向条件概率。表示后发生的事件由先验条件引起的概率。例如已经遇上了堵车，那么堵车由发生交通事故引发的概率就是`P(B|A)`。

**条件概率**：一个事件发生后另一个事件发生的概率。例如已知发生了交通事故，那么此时堵车的概率就是条件概率`P(A|B)`。

贝叶斯公式阐述了后验概率、先验概率和条件概率之间的关系：

$$p(B\, | \,A) = \frac{p(A\, | \, B)p(B)}{p(A)}$$

此外还有条件概率公式：

$$p(A\, | \,B) = \frac{p(AB)}{p(B)}$$

### 贝叶斯估计

***

贝叶斯估计中，认为参数$\theta$也是服从某种概率分布$p(\theta)$的，已有的数据是在这种参数的概率分布下产生的。

$p(x\|D)$被称作后验分布（概率），使用它估计$\theta$有三种常用方法：

- 使用后验分布的密度函数最大值点作为$\theta$的点估计的`最大后验估计 - MAP`，该方法与频率学派的`MLE`方法的不同之处就在于**先验信息**的使用。

- 使用后验分布的中位数作为$\theta$的点估计的后验中位数估计。

- 使用后验分布的均值作为$\theta$的点估计的`后验期望估计`。

用得最多的`后验期望估计`一般直接称为贝叶斯估计。

沿用抛硬币的例子，假设投掷3次硬币，正面记1，概率为$\rho$，反面记0，概率为$(1-\rho)$，得到数据集$D = \{1, 0, 0\}$，$\theta = \rho$。

同样的，可以得到一个概率表达式，与式2完全相同：

式2:

$$p(\theta\, | \,D) = \frac{p(D\, | \,\theta)p(\theta)}{p(D)}$$

在上式中，除了$p(D)$仍然可以看作归一化常数，其余几个因子都是概率分布函数。

此处要引入一个全概率公式的概念。全概率指事件$A$在一系列情况$B_i = \{B_1, B_2, ..., B_N\}$下发生的简单时间的概率的求和（离散）或积分（连续）。

$$P(A) = \sum_{i=1}^N{P(B_i)P(A|B_i)}$$

根据上述公式，可以将$p(D)$展开：

式7:

$$ p(D) = \int_\theta{p(D|\theta)p(\theta)d\theta}$$

回忆一下式4:

$$p(D\, |\,\theta) = \prod_{i=1}^n p(x_i | \theta)$$

将式4和式7共同带入式2，就能得到对贝叶斯估计模型的推导过程。

式8:

$$p(\theta\, | \,D) = \frac{(\prod_{i=1}^n p(x_i | \theta))p(\theta)}{\int_\theta{(\prod_{i=1}^n p(x_i | \theta))p(\theta)d\theta}}$$

##### 结合例子看看

与MLE不同的是，式8中包含先验概率$p(\theta)$，根据抛硬币实验的经验，我们可以假设这个先验概率服从某种分布，比如Beta分布。

![d8b447292d87700cce157ba4c14c5751.png](https://app.yinxiang.com/shard/s8/res/5687477b-09ec-4a8b-8aa0-3ecd9f84d249/45421093-7ec6-4672-95a9-b2c83d222876.png)

将所有已知的式子带入到式8中，就能求出后验概率$p(\theta\|D)$。

### 使用贝叶斯估计的参数做预测

***

利用通过贝叶斯估计得到的后验概率$p(\theta\|D)$，可以求得数据集$D = \{x_1, x_2, ..., x_N\}$的下一个数据点$x_{N+1}$取值的数学期望，即：

式9:

$$E(x|D) = \int_x{x p(x|D)dx}$$

也就是要求$p(x\|D)$，同时又有$x$的分布与参数$\theta$有关，而$\theta$又是服从某种概率分布的，因此有下列式子成立。

式10:

$$p(x|D) = \int_\theta{p(x, \theta | D)d\theta}$$

式11:

$$p(x, \theta | D) = p(x | \theta, D)p(\theta|D)$$

式11代表$x$和$\theta$在已知数据$D$的条件下的概率`=`$x$在已知$\theta$和数据$D$的条件下的概率`*`$\theta$在已知数据$D$的条件下的概率。

而$x$在已知$\theta$和数据$D$的条件下的概率可以化简为：

式12:

$$p(x|\theta, D) = p(x|\theta)$$

因为对于数据$x_{N+1}$来说，数据集$D$和由数据集$D$估计出来的参数$\theta$是同一条件。

因此，可以得到式13如下：

$$p(x|D) = \int_\theta{p(x, \theta | D)d\theta} = \int_\theta{p(x|\theta)p(\theta|D)d\theta}$$

这里的$p(x\|\theta)$，根据投掷硬币的例子，可以是$p(1\|\rho) = \rho$或者$p(0\|\rho) = 1-\rho$。此时就可以求出$x_{N+1}$的数学期望了。

### 最大化后验概率MAP

***

式13的计算难点在于，$\theta$也是随机分布的，因此需要对每一个可能的参数情况积分。为了简化运算，只考虑一个能够最大化后验概率$p(\theta\|D)$的参数$\theta$的情况。

其实就是在MLE求解极大似然函数的式子中，考虑先验概率$p(\theta)$，不考虑$p(D)$

$$\hat{\theta}_{MAP} = arg \max_\theta p(D\, |\,\theta) p(\theta),$$

$$p(D\, |\,\theta) = \prod_{i=1}^n p(x_i | \theta).$$

类似MLE，可以直接求出参数$\theta$的值。

### 贝叶斯参数估计

***

贝叶斯参数估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定的概率分布。

贝叶斯参数估计根据参数的先验分布$p(\theta)$和数据$D$求出$\theta$的后验分布$p(\theta\|D)$，然后求出$\theta$的期望值。

贝叶斯参数估计的本质是通过贝叶斯决策得到参数$\theta$的最优估计，使得总期望风险最小。

![4b9ff8ac7064d7486b65f184f6190a78.png](https://app.yinxiang.com/shard/s8/res/2a6401d1-cd2f-484f-a2e4-f96c026695f9/3804e100-ce1f-4c80-a516-23f7b154b016.png)

这里的损失函数$\lambda(\theta-\hat{\theta})$通常采用方差$(\theta-\hat{\theta})^2$。

在给定数据$D$的情况下，$\theta$的贝叶斯估计值是：

$\hat{\theta} = E[\theta\|D] = \int_\theta{\theta p(\theta\|D)d\theta}$

![c885875e9c5b989d63bcbf54b6c52c25.png](evernotecid://E94EDE04-F978-4D1D-8415-97D907E48B5C/appyinxiangcom/16361109/ENResource/p2387)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY3ODk5OTExOSwtMjEzMDU3OTgyOCwxMD
U0MTU4MzU3LDE2NjA5OTUwMjEsMjg0MjU1Mjk5LDE1ODAzNDk2
MzVdfQ==
-->